{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the Saildrone and MUR global 1 km sea surface temperature collocation code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "\n",
    "def get_sat_filename(date):\n",
    "    dir_sat='F:/data/sst/jpl_mur/v4.1/'\n",
    "    syr, smon, sdym, sjdy = str(date.dt.year.data), str(date.dt.month.data).zfill(2), str(date.dt.day.data).zfill(2), str(date.dt.dayofyear.data).zfill(2)\n",
    "    sat_filename = dir_sat + syr + '/'+ sjdy + '/' + syr + smon + sdym + '090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc'\n",
    "    exists = os.path.isfile(sat_filename)\n",
    "    return sat_filename, exists\n",
    "\n",
    "def robust_std(ds):\n",
    "    MAD = np.nanmedian(ds)\n",
    "    std_robust = MAD * 1.482602218505602"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in USV data\n",
    "Read in the Saildrone USV file either from a local disc or using OpenDAP.\n",
    "\n",
    "There are 6 NaN values in the lat/lon data arrays, interpolate across these\n",
    "\n",
    "We want to collocate with wind vectors for this example,  but the wind vectors are only every 10 minutes rather than every minute, so use .dropna to remove all values in the dataset from all dataarrays when wind vectors aren't availalbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_collocation_data = 'F:/data/cruise_data/saildrone/baja-2018/ccmp_collocation_data.nc'\n",
    "#filename_usv = 'https://podaac-opendap.jpl.nasa.gov/opendap/hyrax/allData/insitu/L2/saildrone/Baja/saildrone-gen_4-baja_2018-sd1002-20180411T180000-20180611T055959-1_minutes-v1.nc'\n",
    "filename_usv='f:/data/cruise_data/saildrone/baja-2018/saildrone-gen_4-baja_2018-sd1002-20180411T180000-20180611T055959-1_minutes-v1.nc'\n",
    "ds_usv = xr.open_dataset(filename_usv)\n",
    "ds_usv.close()\n",
    "ds_usv = ds_usv.isel(trajectory=0).swap_dims({'obs':'time'}).rename({'longitude':'lon','latitude':'lat'})\n",
    "ds_usv = ds_usv.sel(time=slice('2018-04-12T02','2018-06-10T18')) #get rid of last part and first part where USV being towed\n",
    "ds_usv['lon'] = ds_usv.lon.interpolate_na(dim='time',method='linear') #there are 6 nan values\n",
    "ds_usv['lat'] = ds_usv.lat.interpolate_na(dim='time',method='linear')\n",
    "ds_usv['wind_speed']=np.sqrt(ds_usv.UWND_MEAN**2+ds_usv.VWND_MEAN**2)\n",
    "ds_usv['wind_dir']=np.arctan2(ds_usv.VWND_MEAN,ds_usv.UWND_MEAN)*180/np.pi\n",
    "ds_usv_subset = ds_usv.copy(deep=True)\n",
    "ds_usv_subset = ds_usv_subset.where(np.logical_not((ds_usv.time.dt.hour>12)&(ds_usv.wind_speed<7.5)))\n",
    "ds_usv_subset = ds_usv_subset.where(np.logical_not((ds_usv.time.dt.hour<6)&(ds_usv.wind_speed<7.5)))\n",
    "ds_usv_subset = ds_usv_subset.where(np.logical_not((ds_usv.time>np.datetime64('2018-05-24T12')) & (ds_usv.time<np.datetime64('2018-05-26T12'))))\n",
    "#ds_usv_subset = ds_usv.dropna(dim='time',subset={'UWND_MEAN'})   #get rid of all the nan\n",
    "#print(ds_usv_subset.UWND_MEAN[2000:2010].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use open_mfdataset you need to either provide a path or a list of filenames to input\n",
    "\n",
    "Here we use the USV cruise start and end date to read in all data for that period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:/data/sst/jpl_mur/v4.1/2018/102/20180412090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc\n"
     ]
    }
   ],
   "source": [
    "read_date,end_date = ds_usv_subset.time.min(),ds_usv_subset.time.max()\n",
    "filelist = []\n",
    "while read_date<=(end_date+np.timedelta64(1,'D')):\n",
    "    tem_filename,exists = get_sat_filename(read_date)\n",
    "    if exists:\n",
    "        filelist.append(tem_filename)\n",
    "    read_date=read_date+np.timedelta64(1,'D')\n",
    "print(filelist[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in MUR data\n",
    "Read in data using open_mfdataset with the option coords='minimal'\n",
    "\n",
    "The dataset is printed out and you can see that rather than straight xarray data array for each of the data variables open_mfdataset using dask arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:           (lat: 17999, lon: 36000, time: 61)\n",
       "Coordinates:\n",
       "  * lat               (lat) float32 -89.99 -89.98 -89.97 ... 89.97 89.98 89.99\n",
       "  * lon               (lon) float32 -179.99 -179.98 -179.97 ... 179.99 180.0\n",
       "  * time              (time) datetime64[ns] 2018-04-12T09:00:00 ... 2018-06-11T09:00:00\n",
       "Data variables:\n",
       "    analysed_sst      (time, lat, lon) float32 dask.array<shape=(61, 17999, 36000), chunksize=(1, 17999, 36000)>\n",
       "    analysis_error    (time, lat, lon) float32 dask.array<shape=(61, 17999, 36000), chunksize=(1, 17999, 36000)>\n",
       "    mask              (time, lat, lon) float32 dask.array<shape=(61, 17999, 36000), chunksize=(1, 17999, 36000)>\n",
       "    sea_ice_fraction  (time, lat, lon) float32 dask.array<shape=(61, 17999, 36000), chunksize=(1, 17999, 36000)>\n",
       "    dt_1km_data       (time, lat, lon) timedelta64[ns] dask.array<shape=(61, 17999, 36000), chunksize=(1, 17999, 36000)>\n",
       "Attributes:\n",
       "    Conventions:                CF-1.5\n",
       "    title:                      Daily MUR SST, Final product\n",
       "    summary:                    A merged, multi-sensor L4 Foundation SST anal...\n",
       "    references:                 http://podaac.jpl.nasa.gov/Multi-scale_Ultra-...\n",
       "    institution:                Jet Propulsion Laboratory\n",
       "    history:                    created at nominal 4-day latency; replaced nr...\n",
       "    comment:                    MUR = \"Multi-scale Ultra-high Reolution\"\n",
       "    license:                    These data are available free of charge under...\n",
       "    id:                         MUR-JPL-L4-GLOB-v04.1\n",
       "    naming_authority:           org.ghrsst\n",
       "    product_version:            04.1\n",
       "    uuid:                       27665bc0-d5fc-11e1-9b23-0800200c9a66\n",
       "    gds_version_id:             2.0\n",
       "    netcdf_version_id:          4.1\n",
       "    date_created:               20180519T022756Z\n",
       "    start_time:                 20180412T090000Z\n",
       "    stop_time:                  20180412T090000Z\n",
       "    time_coverage_start:        20180411T210000Z\n",
       "    time_coverage_end:          20180412T210000Z\n",
       "    file_quality_level:         1\n",
       "    source:                     MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRR1...\n",
       "    platform:                   Terra, Aqua, GCOM-W, NOAA-19, MetOp-A, Buoys/...\n",
       "    sensor:                     MODIS, AMSR2, AVHRR, in-situ\n",
       "    Metadata_Conventions:       Unidata Observation Dataset v1.0\n",
       "    metadata_link:              http://podaac.jpl.nasa.gov/ws/metadata/datase...\n",
       "    keywords:                   Oceans > Ocean Temperature > Sea Surface Temp...\n",
       "    keywords_vocabulary:        NASA Global Change Master Directory (GCMD) Sc...\n",
       "    standard_name_vocabulary:   NetCDF Climate and Forecast (CF) Metadata Con...\n",
       "    southernmost_latitude:      -90.0\n",
       "    northernmost_latitude:      90.0\n",
       "    westernmost_longitude:      -180.0\n",
       "    easternmost_longitude:      180.0\n",
       "    spatial_resolution:         0.01 degrees\n",
       "    geospatial_lat_units:       degrees north\n",
       "    geospatial_lat_resolution:  0.01 degrees\n",
       "    geospatial_lon_units:       degrees east\n",
       "    geospatial_lon_resolution:  0.01 degrees\n",
       "    acknowledgment:             Please acknowledge the use of these data with...\n",
       "    creator_name:               JPL MUR SST project\n",
       "    creator_email:              ghrsst@podaac.jpl.nasa.gov\n",
       "    creator_url:                http://mur.jpl.nasa.gov\n",
       "    project:                    NASA Making Earth Science Data Records for Us...\n",
       "    publisher_name:             GHRSST Project Office\n",
       "    publisher_url:              http://www.ghrsst.org\n",
       "    publisher_email:            ghrsst-po@nceo.ac.uk\n",
       "    processing_level:           L4\n",
       "    cdm_data_type:              grid"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_sat = xr.open_mfdataset(filelist,coords='minimal')\n",
    "ds_sat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xarray interpolation won't run on chunked dimensions.  \n",
    "1. First let's subset the data to make it smaller to deal with by using the cruise lat/lons\n",
    "\n",
    "1. Now load the data into memory (de-Dask-ify) it  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min max lat lon: -125.55297279999999 -115.5226624 28.0176832 37.6797408\n"
     ]
    }
   ],
   "source": [
    "#Step 1 from above\n",
    "print('min max lat lon:', ds_usv_subset.lon.min().data,ds_usv_subset.lon.max().data,ds_usv_subset.lat.min().data,ds_usv_subset.lat.max().data)\n",
    "subset = ds_sat.sel(lon=slice(ds_usv_subset.lon.min().data,ds_usv_subset.lon.max().data),\n",
    "                    lat=slice(ds_usv_subset.lat.min().data,ds_usv_subset.lat.max().data))\n",
    "#Step 2 from above\n",
    "subset.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocate USV data with MUR data\n",
    "There are different options when you interpolate.  First, let's just do a linear interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds_collocated = subset.interp(lat=ds_usv_subset.lat,lon=ds_usv_subset.lon,time=ds_usv_subset.time,method='linear')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocate USV data with MUR data\n",
    "There are different options when you interpolate.  First, let's just do a nearest point rather than interpolate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_collocated_nearest = subset.interp(lat=ds_usv_subset.lat,lon=ds_usv_subset.lon,time=ds_usv_subset.time,method='nearest')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A larger STD that isn't reflective of uncertainty in the observation\n",
    "The collocation above will result in multiple USV data points matched with a single satellite\n",
    "observation.    The USV is sampling every 1 min and approximately few meters, while the satellite\n",
    "is an average over a footprint that is interpolated onto a daily mean map.  While calculating the mean would results in a valid mean, the STD would be higher and consist of a component that reflects the uncertainty of the USV and the satellite and a component that reflects the natural variability in the region that is sampled by the USV\n",
    "\n",
    "Below we use the 'nearest' collocation results to identify when multiple USV data are collcated to\n",
    "a single satellite observation.\n",
    "This code goes through the data and creates averages of the USV data that match the single CCMP collocated value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ilen,index = ds_collocated_nearest.dims['time'],0\n",
    "ds_tem = ds_collocated_nearest.copy(deep=True)\n",
    "duu, duv1, duv2, dlat, dlon, dut = [],[],[],[],[],np.empty((),dtype='datetime64')\n",
    "while index <= ilen-2:\n",
    "    index += 1\n",
    "    if np.isnan(ds_collocated_nearest.analysed_sst[index]):\n",
    "        continue\n",
    "    if np.isnan(ds_tem.analysed_sst[index]):\n",
    "        continue\n",
    "   # print(index, ilen)\n",
    "    iend = index + 1000\n",
    "    if iend > ilen-1:\n",
    "        iend = ilen-1\n",
    "    ds_tem_subset = ds_tem.analysed_sst[index:iend]\n",
    "    ds_usv_subset2sst = ds_usv_subset.TEMP_CTD_MEAN[index:iend]\n",
    "    ds_usv_subset2uwnd = ds_usv_subset.UWND_MEAN[index:iend]\n",
    "    ds_usv_subset2vwnd = ds_usv_subset.VWND_MEAN[index:iend]\n",
    "    ds_usv_subset2lat = ds_usv_subset.lat[index:iend]\n",
    "    ds_usv_subset2lon = ds_usv_subset.lon[index:iend]\n",
    "    ds_usv_subset2time = ds_usv_subset.time[index:iend]\n",
    "    cond = ((ds_tem_subset==ds_collocated_nearest.analysed_sst[index]))\n",
    "    notcond = np.logical_not(cond)\n",
    "    #cond = ((ds_tem.analysed_sst==ds_collocated_nearest.analysed_sst[index]))\n",
    "    #notcond = np.logical_not(cond)\n",
    "    masked = ds_tem_subset.where(cond)\n",
    "    if masked.sum().data==0:  #don't do if data not found\n",
    "        continue\n",
    "    masked_usvsst = ds_usv_subset2sst.where(cond,drop=True)\n",
    "    masked_usvuwnd = ds_usv_subset2uwnd.where(cond,drop=True)\n",
    "    masked_usvvwnd = ds_usv_subset2vwnd.where(cond,drop=True)\n",
    "    masked_usvlat = ds_usv_subset2lat.where(cond,drop=True)\n",
    "    masked_usvlon = ds_usv_subset2lon.where(cond,drop=True)\n",
    "    masked_usvtime = ds_usv_subset2time.where(cond,drop=True)\n",
    "    duu=np.append(duu,masked_usvsst.mean().data)\n",
    "    duv1=np.append(duv1,masked_usvuwnd.mean().data)\n",
    "    duv2=np.append(duv2,masked_usvvwnd.mean().data)\n",
    "    dlat=np.append(dlat,masked_usvlat.mean().data)\n",
    "    dlon=np.append(dlon,masked_usvlon.mean().data)\n",
    "    tdif = masked_usvtime[-1].data-masked_usvtime[0].data\n",
    "    mtime=masked_usvtime[0].data+np.timedelta64(tdif/2,'ns')\n",
    "    if mtime>dut.max():\n",
    "        print(index,dut.shape[0],masked_usvtime[0].data,masked_usvtime[-1].data-masked_usvtime[0].data)\n",
    "    dut=np.append(dut,mtime)\n",
    "    ds_tem.analysed_sst[index:iend]=ds_tem.analysed_sst.where(notcond)\n",
    "#    ds_tem=ds_tem.where(notcond,np.nan)  #masked used values by setting to nan\n",
    "dut2 = dut[1:]  #remove first data point which is a repeat from what array defined    \n",
    "ds_new=xr.Dataset(data_vars={'sst_usv': ('time',duu),'uwnd_usv': ('time',duv1),'vwnd_usv': ('time',duv2),\n",
    "                             'lon': ('time',dlon),\n",
    "                             'lat': ('time',dlat)},\n",
    "                  coords={'time':dut2})\n",
    "ds_new.to_netcdf('F:/data/cruise_data/saildrone/baja-2018/mur_downsampled_usv_data2.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# redo the collocation\n",
    "Now, redo the collocation, using 'linear' interpolation using the averaged data.  This will interpolate the data temporally onto the USV sampling which has been averaged to the satellite data grid points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-11T00:02:00.000000000\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:           (time: 6075)\n",
       "Coordinates:\n",
       "    lat               (time) float64 37.81 37.81 37.81 ... 37.78 37.77 37.77\n",
       "    lon               (time) float64 -122.5 -122.5 -122.5 ... -122.3 -122.3\n",
       "  * time              (time) datetime64[ns] 2018-04-11T18:53:30 ... 2018-06-11T00:02:00\n",
       "Data variables:\n",
       "    analysed_sst      (time) float64 286.3 286.3 286.3 286.3 ... nan nan nan\n",
       "    analysis_error    (time) float64 0.3899 0.3884 0.3865 0.3858 ... nan nan nan\n",
       "    mask              (time) float64 1.0 1.0 1.0 1.0 ... 1.0 1.009 1.464 1.447\n",
       "    sea_ice_fraction  (time) float64 nan nan nan nan nan ... nan nan nan nan nan\n",
       "Attributes:\n",
       "    Conventions:                CF-1.5\n",
       "    title:                      Daily MUR SST, Final product\n",
       "    summary:                    A merged, multi-sensor L4 Foundation SST anal...\n",
       "    references:                 http://podaac.jpl.nasa.gov/Multi-scale_Ultra-...\n",
       "    institution:                Jet Propulsion Laboratory\n",
       "    history:                    created at nominal 4-day latency; replaced nr...\n",
       "    comment:                    MUR = \"Multi-scale Ultra-high Reolution\"\n",
       "    license:                    These data are available free of charge under...\n",
       "    id:                         MUR-JPL-L4-GLOB-v04.1\n",
       "    naming_authority:           org.ghrsst\n",
       "    product_version:            04.1\n",
       "    uuid:                       27665bc0-d5fc-11e1-9b23-0800200c9a66\n",
       "    gds_version_id:             2.0\n",
       "    netcdf_version_id:          4.1\n",
       "    date_created:               20180519T013315Z\n",
       "    start_time:                 20180411T090000Z\n",
       "    stop_time:                  20180411T090000Z\n",
       "    time_coverage_start:        20180410T210000Z\n",
       "    time_coverage_end:          20180411T210000Z\n",
       "    file_quality_level:         1\n",
       "    source:                     MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRR1...\n",
       "    platform:                   Terra, Aqua, GCOM-W, NOAA-19, MetOp-A, Buoys/...\n",
       "    sensor:                     MODIS, AMSR2, AVHRR, in-situ\n",
       "    Metadata_Conventions:       Unidata Observation Dataset v1.0\n",
       "    metadata_link:              http://podaac.jpl.nasa.gov/ws/metadata/datase...\n",
       "    keywords:                   Oceans > Ocean Temperature > Sea Surface Temp...\n",
       "    keywords_vocabulary:        NASA Global Change Master Directory (GCMD) Sc...\n",
       "    standard_name_vocabulary:   NetCDF Climate and Forecast (CF) Metadata Con...\n",
       "    southernmost_latitude:      -90.0\n",
       "    northernmost_latitude:      90.0\n",
       "    westernmost_longitude:      -180.0\n",
       "    easternmost_longitude:      180.0\n",
       "    spatial_resolution:         0.01 degrees\n",
       "    geospatial_lat_units:       degrees north\n",
       "    geospatial_lat_resolution:  0.01 degrees\n",
       "    geospatial_lon_units:       degrees east\n",
       "    geospatial_lon_resolution:  0.01 degrees\n",
       "    acknowledgment:             Please acknowledge the use of these data with...\n",
       "    creator_name:               JPL MUR SST project\n",
       "    creator_email:              ghrsst@podaac.jpl.nasa.gov\n",
       "    creator_url:                http://mur.jpl.nasa.gov\n",
       "    project:                    NASA Making Earth Science Data Records for Us...\n",
       "    publisher_name:             GHRSST Project Office\n",
       "    publisher_url:              http://www.ghrsst.org\n",
       "    publisher_email:            ghrsst-po@nceo.ac.uk\n",
       "    processing_level:           L4\n",
       "    cdm_data_type:              grid"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_collocated_averaged = subset.interp(lat=ds_new.lat,lon=ds_new.lon,time=ds_new.time,method='linear')\n",
    "ds_collocated_averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_collocated_averaged.to_netcdf('F:/data/cruise_data/saildrone/baja-2018/mur_downsampled_collocated_usv_data2.nc')\n",
    "ds_new.to_netcdf('F:/data/cruise_data/saildrone/baja-2018/mur_downsampled_usv_data2.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'time' (time: 200)>\n",
      "array(['2018-06-10T01:32:00.000000000', '2018-06-10T01:27:00.000000000',\n",
      "       '2018-06-10T01:31:30.000000000', '2018-06-10T01:36:30.000000000',\n",
      "       '2018-06-10T01:42:30.000000000', '2018-06-10T01:53:00.000000000',\n",
      "       '2018-06-10T02:07:00.000000000', '2018-06-10T03:03:00.000000000',\n",
      "       '2018-06-10T02:29:30.000000000', '2018-06-10T02:36:00.000000000',\n",
      "       '2018-06-10T03:38:00.000000000', '2018-06-10T02:57:00.000000000',\n",
      "       '2018-06-10T02:52:00.000000000', '2018-06-10T03:38:00.000000000',\n",
      "       '2018-06-10T03:25:30.000000000', '2018-06-10T03:16:30.000000000',\n",
      "       '2018-06-10T03:21:30.000000000', '2018-06-10T03:25:00.000000000',\n",
      "       '2018-06-10T04:05:30.000000000', '2018-06-10T03:48:00.000000000',\n",
      "       '2018-06-10T03:53:00.000000000', '2018-06-10T03:57:30.000000000',\n",
      "       '2018-06-10T04:13:00.000000000', '2018-06-10T04:08:00.000000000',\n",
      "       '2018-06-10T04:12:00.000000000', '2018-06-10T04:52:00.000000000',\n",
      "       '2018-06-10T04:51:30.000000000', '2018-06-10T05:02:30.000000000',\n",
      "       '2018-06-10T05:18:00.000000000', '2018-06-10T05:22:00.000000000',\n",
      "       '2018-06-10T05:33:00.000000000', '2018-06-10T05:37:00.000000000',\n",
      "       '2018-06-10T05:43:30.000000000', '2018-06-10T05:47:30.000000000',\n",
      "       '2018-06-10T05:52:30.000000000', '2018-06-10T06:09:00.000000000',\n",
      "       '2018-06-10T06:14:00.000000000', '2018-06-10T06:18:00.000000000',\n",
      "       '2018-06-10T06:25:00.000000000', '2018-06-10T06:29:00.000000000',\n",
      "       '2018-06-10T06:33:30.000000000', '2018-06-10T06:39:30.000000000',\n",
      "       '2018-06-10T06:43:30.000000000', '2018-06-10T06:54:30.000000000',\n",
      "       '2018-06-10T06:58:30.000000000', '2018-06-10T07:05:00.000000000',\n",
      "       '2018-06-10T07:09:00.000000000', '2018-06-10T07:13:30.000000000',\n",
      "       '2018-06-10T07:19:30.000000000', '2018-06-10T07:23:30.000000000',\n",
      "       '2018-06-10T07:29:30.000000000', '2018-06-10T07:34:30.000000000',\n",
      "       '2018-06-10T07:38:30.000000000', '2018-06-10T07:45:30.000000000',\n",
      "       '2018-06-10T07:53:30.000000000', '2018-06-10T08:00:00.000000000',\n",
      "       '2018-06-10T08:04:00.000000000', '2018-06-10T08:09:30.000000000',\n",
      "       '2018-06-10T08:15:30.000000000', '2018-06-10T08:20:00.000000000',\n",
      "       '2018-06-10T08:26:30.000000000', '2018-06-10T08:31:00.000000000',\n",
      "       '2018-06-10T08:35:30.000000000', '2018-06-10T08:42:30.000000000',\n",
      "       '2018-06-10T08:47:00.000000000', '2018-06-10T08:52:30.000000000',\n",
      "       '2018-06-10T08:58:30.000000000', '2018-06-10T09:03:00.000000000',\n",
      "       '2018-06-10T09:09:30.000000000', '2018-06-10T09:14:00.000000000',\n",
      "       '2018-06-10T09:18:30.000000000', '2018-06-10T09:26:30.000000000',\n",
      "       '2018-06-10T09:31:00.000000000', '2018-06-10T09:36:00.000000000',\n",
      "       '2018-06-10T09:43:00.000000000', '2018-06-10T09:47:30.000000000',\n",
      "       '2018-06-10T09:54:00.000000000', '2018-06-10T09:59:30.000000000',\n",
      "       '2018-06-10T10:04:00.000000000', '2018-06-10T10:11:30.000000000',\n",
      "       '2018-06-10T10:16:00.000000000', '2018-06-10T10:20:30.000000000',\n",
      "       '2018-06-10T10:27:30.000000000', '2018-06-10T10:32:00.000000000',\n",
      "       '2018-06-10T10:37:30.000000000', '2018-06-10T10:43:00.000000000',\n",
      "       '2018-06-10T10:47:00.000000000', '2018-06-10T10:54:00.000000000',\n",
      "       '2018-06-10T11:02:00.000000000', '2018-06-10T11:09:00.000000000',\n",
      "       '2018-06-10T11:13:00.000000000', '2018-06-10T11:17:30.000000000',\n",
      "       '2018-06-10T11:23:30.000000000', '2018-06-10T11:27:30.000000000',\n",
      "       '2018-06-10T11:34:00.000000000', '2018-06-10T11:39:30.000000000',\n",
      "       '2018-06-10T11:43:30.000000000', '2018-06-10T11:50:30.000000000',\n",
      "       '2018-06-10T11:58:30.000000000', '2018-06-10T12:05:30.000000000',\n",
      "       '2018-06-10T12:09:30.000000000', '2018-06-10T12:14:30.000000000',\n",
      "       '2018-06-10T12:20:00.000000000', '2018-06-10T14:55:00.000000000',\n",
      "       '2018-06-10T12:30:30.000000000', '2018-06-10T12:35:00.000000000',\n",
      "       '2018-06-10T12:39:00.000000000', '2018-06-10T12:45:00.000000000',\n",
      "       '2018-06-10T12:48:30.000000000', '2018-06-10T12:53:30.000000000',\n",
      "       '2018-06-10T12:59:30.000000000', '2018-06-10T17:15:00.000000000',\n",
      "       '2018-06-10T13:10:30.000000000', '2018-06-10T17:27:30.000000000',\n",
      "       '2018-06-10T13:19:30.000000000', '2018-06-10T13:26:30.000000000',\n",
      "       '2018-06-10T13:31:00.000000000', '2018-06-10T13:35:00.000000000',\n",
      "       '2018-06-10T13:41:30.000000000', '2018-06-10T15:54:00.000000000',\n",
      "       '2018-06-10T15:51:00.000000000', '2018-06-10T13:57:00.000000000',\n",
      "       '2018-06-10T14:01:30.000000000', '2018-06-10T14:08:30.000000000',\n",
      "       '2018-06-10T14:13:00.000000000', '2018-06-10T14:17:30.000000000',\n",
      "       '2018-06-10T14:24:00.000000000', '2018-06-10T14:28:00.000000000',\n",
      "       '2018-06-10T14:34:00.000000000', '2018-06-10T14:43:00.000000000',\n",
      "       '2018-06-10T14:52:30.000000000', '2018-06-10T15:49:30.000000000',\n",
      "       '2018-06-10T15:11:00.000000000', '2018-06-10T15:16:30.000000000',\n",
      "       '2018-06-10T15:52:00.000000000', '2018-06-10T15:34:00.000000000',\n",
      "       '2018-06-10T15:47:30.000000000', '2018-06-10T15:45:30.000000000',\n",
      "       '2018-06-10T16:01:00.000000000', '2018-06-10T16:08:00.000000000',\n",
      "       '2018-06-10T16:12:30.000000000', '2018-06-10T16:19:00.000000000',\n",
      "       '2018-06-10T16:28:30.000000000', '2018-06-10T16:35:30.000000000',\n",
      "       '2018-06-10T16:47:00.000000000', '2018-06-10T18:56:30.000000000',\n",
      "       '2018-06-10T16:58:00.000000000', '2018-06-10T17:05:00.000000000',\n",
      "       '2018-06-10T17:09:30.000000000', '2018-06-10T19:04:30.000000000',\n",
      "       '2018-06-10T17:21:00.000000000', '2018-06-10T17:32:00.000000000',\n",
      "       '2018-06-10T17:36:30.000000000', '2018-06-10T17:42:00.000000000',\n",
      "       '2018-06-10T17:48:00.000000000', '2018-06-10T19:17:00.000000000',\n",
      "       '2018-06-10T18:09:00.000000000', '2018-06-10T20:09:30.000000000',\n",
      "       '2018-06-10T20:06:30.000000000', '2018-06-10T18:27:30.000000000',\n",
      "       '2018-06-10T18:34:00.000000000', '2018-06-10T19:56:30.000000000',\n",
      "       '2018-06-10T18:46:00.000000000', '2018-06-10T19:03:00.000000000',\n",
      "       '2018-06-10T18:59:30.000000000', '2018-06-10T19:02:30.000000000',\n",
      "       '2018-06-10T19:19:00.000000000', '2018-06-10T19:23:00.000000000',\n",
      "       '2018-06-10T19:38:00.000000000', '2018-06-10T19:35:00.000000000',\n",
      "       '2018-06-10T19:39:00.000000000', '2018-06-10T19:45:30.000000000',\n",
      "       '2018-06-10T19:56:00.000000000', '2018-06-10T20:02:30.000000000',\n",
      "       '2018-06-10T20:07:00.000000000', '2018-06-10T20:13:00.000000000',\n",
      "       '2018-06-10T20:22:00.000000000', '2018-06-10T20:33:30.000000000',\n",
      "       '2018-06-10T20:40:30.000000000', '2018-06-10T20:46:00.000000000',\n",
      "       '2018-06-10T20:52:30.000000000', '2018-06-10T21:04:00.000000000',\n",
      "       '2018-06-10T21:33:30.000000000', '2018-06-10T21:58:30.000000000',\n",
      "       '2018-06-10T22:11:30.000000000', '2018-06-10T22:21:00.000000000',\n",
      "       '2018-06-10T22:27:30.000000000', '2018-06-10T22:32:30.000000000',\n",
      "       '2018-06-10T23:27:00.000000000', '2018-06-10T23:29:00.000000000',\n",
      "       '2018-06-10T23:31:00.000000000', '2018-06-10T23:35:00.000000000',\n",
      "       '2018-06-10T23:39:30.000000000', '2018-06-10T23:43:30.000000000',\n",
      "       '2018-06-10T23:46:30.000000000', '2018-06-10T23:48:30.000000000',\n",
      "       '2018-06-10T23:51:30.000000000', '2018-06-10T23:55:30.000000000',\n",
      "       '2018-06-10T23:59:30.000000000', '2018-06-11T00:02:00.000000000'],\n",
      "      dtype='datetime64[ns]')\n",
      "Coordinates:\n",
      "    lat      (time) float64 36.93 36.93 36.94 36.94 ... 37.78 37.78 37.77 37.77\n",
      "    lon      (time) float64 -123.9 -124.0 -123.9 -123.9 ... -122.3 -122.3 -122.3\n",
      "  * time     (time) datetime64[ns] 2018-06-10T01:32:00 ... 2018-06-11T00:02:00\n"
     ]
    }
   ],
   "source": [
    "print(ds_collocated_averaged.time[-200:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_collocated_averaged = xr.open_dataset('F:/data/cruise_data/saildrone/baja-2018/mur_downsampled_collocated_usv_data2.nc')\n",
    "ds_collocated_averaged.close()\n",
    "ds_new = xr.open_dataset('F:/data/cruise_data/saildrone/baja-2018/mur_downsampled_usv_data2.nc')\n",
    "ds_new.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different ways to select data using an Xarray dataset.\n",
    "- The easiest ways are to use .isel or .sel\n",
    "- .isel selects by integer\n",
    "- .sel selects by label\n",
    "\n",
    "A note of caution, if you are using .isel it is better to rename your data variable.  If you run the block of code that selects the data more than once, using .sel it would still have the same result, while using .isel would apply the selection again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_collocated_averaged_subset = ds_collocated_averaged.isel(time=slice(0,-19))\n",
    "ds_new_subset = ds_new.isel(time=slice(0,-19))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"cannot represent labeled-based slice indexer for dimension 'time' with a slice over integer positions; the index is unsorted or non-unique\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-b742697e6dee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mds_collocated_averaged_subset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds_collocated_averaged\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'2018-04-11T18:53:30'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'2018-06-10T16:35:30'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mds_new_subset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds_new\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'2018-04-11T18:53:30'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'2018-06-10T16:35:30'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\newenv\\lib\\site-packages\\xarray\\core\\dataset.py\u001b[0m in \u001b[0;36msel\u001b[1;34m(self, indexers, method, tolerance, drop, **indexers_kwargs)\u001b[0m\n\u001b[0;32m   1729\u001b[0m         \u001b[0mindexers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meither_dict_or_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexers_kwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1730\u001b[0m         pos_indexers, new_indexes = remap_label_indexers(\n\u001b[1;32m-> 1731\u001b[1;33m             self, indexers=indexers, method=method, tolerance=tolerance)\n\u001b[0m\u001b[0;32m   1732\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpos_indexers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1733\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_overwrite_indexes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_indexes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\newenv\\lib\\site-packages\\xarray\\core\\coordinates.py\u001b[0m in \u001b[0;36mremap_label_indexers\u001b[1;34m(obj, indexers, method, tolerance, **indexers_kwargs)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m     pos_indexers, new_indexes = indexing.remap_label_indexers(\n\u001b[1;32m--> 317\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_indexers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m     )\n\u001b[0;32m    319\u001b[0m     \u001b[1;31m# attach indexer's coordinate to pos_indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\newenv\\lib\\site-packages\\xarray\\core\\indexing.py\u001b[0m in \u001b[0;36mremap_label_indexers\u001b[1;34m(data_obj, indexers, method, tolerance)\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m             idxr, new_idx = convert_label_indexer(index, label,\n\u001b[1;32m--> 252\u001b[1;33m                                                   dim, method, tolerance)\n\u001b[0m\u001b[0;32m    253\u001b[0m             \u001b[0mpos_indexers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midxr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnew_idx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\newenv\\lib\\site-packages\\xarray\\core\\indexing.py\u001b[0m in \u001b[0;36mconvert_label_indexer\u001b[1;34m(index, label, index_name, method, tolerance)\u001b[0m\n\u001b[0;32m    141\u001b[0m             raise KeyError('cannot represent labeled-based slice indexer for '\n\u001b[0;32m    142\u001b[0m                            \u001b[1;34m'dimension %r with a slice over integer positions; '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m                            'the index is unsorted or non-unique' % index_name)\n\u001b[0m\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mis_dict_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"cannot represent labeled-based slice indexer for dimension 'time' with a slice over integer positions; the index is unsorted or non-unique\""
     ]
    }
   ],
   "source": [
    "ds_collocated_averaged_subset = ds_collocated_averaged.sel(time=slice('2018-04-11T18:53:30','2018-06-10T16:35:30'))\n",
    "ds_new_subset = ds_new.sel(time=slice('2018-04-11T18:53:30','2018-06-10T16:35:30'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#sat_sst = ds_collocated_averaged.analysed_sst[:-19]-273.15\n",
    "#usv_sst = ds_new.sst_usv[:-19]\n",
    "\n",
    "\n",
    "ds_new['spd']=np.sqrt(ds_new.uwnd_usv**2+ds_new.vwnd_usv**2)\n",
    "usv_spd = ds_new.spd[:-19]\n",
    "dif_sst = (sat_sst - usv_sst).dropna(dim='time')\n",
    "std_robust = np.nanmedian(dif_sst) * 1.482602218505602\n",
    "print('mean,std,rstd, dif ',[dif_sst.mean().data,dif_sst.std().data,std_robust,dif_sst.shape[0]])\n",
    "plt.plot(usv_spd,dif_sst,'.')\n",
    "plt.xlabel('USV wind speed (ms$^{-1}$)')\n",
    "plt.ylabel('USV - Sat SST (K)')\n",
    "sat_sst = ds_collocated_averaged.analysed_sst[:-19]-273.15\n",
    "usv_sst = ds_new.sst_usv[:-19]\n",
    "dif_sst = sat_sst - usv_sst\n",
    "cond = usv_spd>2\n",
    "dif_sst = dif_sst.where(cond).dropna('time')\n",
    "std_robust = np.nanmedian(dif_sst) * 1.482602218505602\n",
    "print('no low wind mean,std,rstd, dif ',[dif_sst.mean().data,dif_sst.std().data,std_robust,sum(cond).data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(usv_sst,dif_sst,'.')\n",
    "plt.xlabel('USV  SST (K)')\n",
    "plt.ylabel('USV - Sat SST (K)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.coastlines()\n",
    "ax.plt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,4))\n",
    "ax.plot(sat_sst,sat_sst-usv_sst,'.')\n",
    "ax.set_xlabel('USV wind speed (ms$^{-1}$)')\n",
    "ax.set_ylabel('USV - Sat wind direction (deg)')\n",
    "fig_fname='F:/data/cruise_data/saildrone/baja-2018/figs/sat_sst_both_bias.png'\n",
    "fig.savefig(fig_fname, transparent=False, format='png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dif_sst[:-19],'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.analysed_sst[0,:,:].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.analysed_sst.mean({'lat','lon'}).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.analysed_sst.mean({'time'}).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.analysed_sst.std({'time'}).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
